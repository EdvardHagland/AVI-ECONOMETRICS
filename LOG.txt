 # CHUNK: setup
> # PURPOSE: Load necessary libraries, configure global options, and read data.
> # FUN NOTE: This setup drove me crazy, but here we are!
> knitr::opts_chunk$set(
+   fig.pos = "H",
+   out.extra = "",
+   fig.height = 4,
+   fig.width = 7,
+   fig.align = "center",
+   out.width = "100%"
+ )
> 
> # Core libraries: data manipulation and tables
> library(tidyverse)
> library(readr)
> library(knitr)
> # Tables and summaries
> library(modelsummary)
> library(gtsummary)
> library(kableExtra)
> # Plotting and modeling
> library(ggplot2)
> library(Hmisc)
> library(mediation)
> library(broom)
> library(RColorBrewer)
> library(patchwork)
> 
> # Numeric formatting for LaTeX tables
> options(modelsummary_format_numeric_latex = "plain")
> datasummary_format_numeric_latex = "plain"
> 
> # Data path definition
> data_path <- "../data/"
> 
> # Load data frames
> mig_wide <- read_csv(file.path(data_path, "mig.wide.csv"), show_col_types = FALSE)
> mig_long <- read_csv(file.path(data_path, "mig.long.csv"), show_col_types = FALSE)
> # CHUNK: initial-histograms
> # PURPOSE: Visual exploration of variable distributions through histograms.
> # EXECUTION: Create and arrange histograms for age, eligibility score, and migration aspiration.
> # FUN NOTE: I swear those histograms multiplied overnight.
> 
> # Define a consistent theme for all plots
> plot_theme <- theme_minimal() +
+   theme(
+     plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
+     plot.subtitle = element_text(hjust = 0.5, size = 12),
+     axis.title = element_text(size = 12),
+     axis.text = element_text(size = 10),
+     legend.title = element_text(face = "bold"),
+     legend.position = "top",
+     panel.grid.minor = element_blank(),
+     panel.border = element_rect(color = "gray90", fill = NA, linewidth = 0.5)
+   )
> 
> # Create a color palette
> hist_colors <- c("#2c7fb8", "#7fcdbb", "#41ab5d")
> 
> # 1. Age Distribution (from mig_wide)
> plot_age <- ggplot(mig_wide, aes(x = age)) +
+   geom_histogram(binwidth = 2, fill = hist_colors[1], color = "white", alpha = 0.9) +
+   labs(title = "Distribution of Participant Age",
+        subtitle = "At Baseline (Wave 1)",
+        x = "Age (years)",
+        y = "Frequency") +
+   plot_theme
> 
> # 2. Eligibility Score Distribution (from mig_wide)
> plot_score <- ggplot(mig_wide, aes(x = Score)) +
+   geom_histogram(binwidth = 1, fill = hist_colors[2], color = "white", alpha = 0.9) +
+   labs(title = "Distribution of Eligibility Score",
+        subtitle = "Program Selection Criteria",
+        x = "Eligibility Score",
+        y = "Frequency") +
+   plot_theme
> 
> # 3. Migration Aspiration Distribution Over Time (using mig_long)
> # Ensure mig_long exists and has the required columns
> plot_mgasp_time <- NULL
> if (exists("mig_long") && all(c("mg.asp", "wave", "treat.dum") %in% names(mig_long))) {
+   
+   # Prepare data: Add descriptive labels for wave and treatment
+   plot_data_mgasp <- mig_long %>%
+     mutate(
+       Wave = factor(paste("Wave", wave)),
+       Treatment = factor(treat.dum, levels = c(0, 1), labels = c("Control", "Treatment"))
+     ) %>%
+     filter(!is.na(mg.asp)) # Remove NAs for plotting
+ 
+   plot_mgasp_time <- ggplot(plot_data_mgasp, aes(x = mg.asp)) +
+     # Use geom_bar for discrete/ordinal data, position="dodge" to compare T/C
+     geom_bar(aes(fill = Treatment), position = position_dodge(preserve = "single"), alpha = 0.9) + 
+     facet_wrap(~ Wave, ncol = 1) + # Facet by wave
+     scale_fill_brewer(palette = "Set1", name = "Group") + # Better color scheme
+     labs(title = "Distribution of Migration Aspiration",
+          subtitle = "By Wave and Treatment Group",
+          x = "Migration Aspiration (1=Low, 5=High)",
+          y = "Count") +
+     plot_theme +
+     theme(
+       strip.text = element_text(face = "bold", size = 12),
+       strip.background = element_rect(fill = "gray95")
+     ) +
+     scale_x_continuous(breaks = 1:5) # Ensure x-axis shows integer levels 1-5
+ }
> 
> # Arrange plots in a grid using patchwork
> if (!is.null(plot_mgasp_time)) {
+   # If we have all three plots, arrange them in a 2x2 grid with mgasp_time spanning two rows
+   combined_plot <- (plot_age + plot_score) / 
+                    plot_mgasp_time + 
+                    plot_layout(heights = c(1, 2))
+   
+   # Add a title to the combined plot
+   combined_plot <- combined_plot + 
+     plot_annotation(
+       title = "Initial Data Exploration: Key Variables",
+       theme = theme(
+         plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
+         plot.margin = margin(t = 15, r = 10, b = 10, l = 10)
+       )
+     )
+   
+   # Print the combined plot
+   print(combined_plot)
+ } else {
+   # If migration aspiration plot couldn't be created, combine just the two plots
+   if (exists("mig_wide") && all(c("age", "Score") %in% names(mig_wide))) {
+     combined_plot <- plot_age + plot_score +
+       plot_annotation(
+         title = "Initial Data Exploration: Key Variables",
+         theme = theme(
+           plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
+           plot.margin = margin(t = 15, r = 10, b = 10, l = 10)
+         )
+       )
+     print(combined_plot)
+   } else {
+      message("Skipping histograms: Required data not found.")
+   }
+ }
> # CHUNK: table1-replication
> # PURPOSE: Calculate weighted means and F-tests for key business outcomes; render formatted LaTeX table.
> # EXECUTION: Iterate over outcome variables, compute stats, apply labels, and use kableExtra.
> # FUN NOTE: I ran this chunk praying to the statistical deities.
> 
> # --- INTENT: Prepare the wide-format data for analysis by creating a factor variable for treatment status. ---
> # --- EFFECT: Takes the `mig_wide` tibble, uses `mutate` to create a new column `treat.dum_fac`. This new column converts the numeric `treat.dum` (0/1) into a factor with meaningful labels ("Control", "Treatment"). Assigns the result to `mig_business`. ---
> # Prepare the data
> mig_business <- mig_wide %>%
+   # Create factor for treatment
+   mutate(treat.dum_fac = factor(treat.dum, levels = c(0, 1), labels = c("Control", "Treatment")))
> 
> # --- INTENT: Define the specific business outcome variables (columns in `mig_wide`) to be analyzed for Table 1. ---
> # --- EFFECT: Creates a character vector `business_vars` containing the exact names of the columns representing the business outcomes measured in Wave 3. ---
> # Business outcome variables for Wave 3
> business_vars <- c("bus.location", "bus.open", "bus.purchases", "bus.hire", "bus.profit", "bus.close")
> 
> # --- INTENT: Create a dataset containing only necessary columns for Table 1, mirroring original script. ---
> # --- EFFECT: Selects only business outcome variables, treatment factor, and wave 3 weights, without pre-filtering rows. Assigns to `table1_data`. ---
> # Create data subset for Table 1 (no row filtering yet)
> table1_data <- mig_business %>% # Start with mig_business which has treat.dum_fac
+   dplyr::select(all_of(business_vars), treat.dum, treat.dum_fac, wt3) # Keep numeric treat.dum for lm
> 
> # --- INTENT: Calculate the weighted means and F-test by treatment group, handling NAs within the loop. ---
> results <- lapply(business_vars, function(var) {
+ 
+   # --- Calculate Weighted Means with Explicit NA Handling for Weights & Outcome ---
+   # Filter base data further to remove NAs in the specific outcome variable OR the weight
+   valid_data_for_mean <- table1_data %>% # Use the unfiltered subset
+     filter(!is.na(.data[[var]]) & !is.na(wt3))
+ 
+   # Calculate mean for Control group if valid data exists
+   control_data <- valid_data_for_mean %>% filter(treat.dum == 0)
+   control_mean <- if (nrow(control_data) > 0) {
+     weighted.mean(control_data[[var]], w = control_data$wt3, na.rm = FALSE) # NAs already removed
+   } else { NA }
+ 
+   # Calculate mean for Treatment group if valid data exists
+   treat_data <- valid_data_for_mean %>% filter(treat.dum == 1)
+   treat_mean <- if (nrow(treat_data) > 0) {
+     weighted.mean(treat_data[[var]], w = treat_data$wt3, na.rm = FALSE) # NAs already removed
+   } else { NA }
+ 
+   # --- Calculate F-test using lm's default NA handling on the unfiltered subset ---
+   # lm will implicitly handle NAs in outcome, predictor, or weight
+   model <- tryCatch({
+     lm(as.formula(paste(var, "~ treat.dum")),
+        data = table1_data, # Use the unfiltered subset
+        weights = wt3)
+   }, error = function(e) NULL) # Return NULL if lm fails
+ 
+   # --- Extract Results ---
+   f_value <- NA
+   p_value <- NA
+   if (!is.null(model)) {
+     f_stat_summary <- summary(model)$fstatistic
+     if (!is.null(f_stat_summary) && length(f_stat_summary) >= 3) {
+       f_value <- f_stat_summary[1]
+       df_num <- f_stat_summary[2]
+       df_den <- f_stat_summary[3]
+       if (!is.na(f_value) && !is.na(df_num) && !is.na(df_den) && df_den > 0) {
+         p_value <- pf(f_value, df_num, df_den, lower.tail = FALSE)
+       }
+     }
+   }
+ 
+   sig_marker <- case_when(
+     is.na(p_value) ~ "",
+     p_value < 0.001 ~ "***",
+     p_value < 0.01 ~ "**",
+     p_value < 0.05 ~ "*",
+     p_value < 0.1 ~ "â€ ",
+     TRUE ~ ""
+   )
+ 
+   f_test <- if (!is.na(f_value)) paste0(round(f_value, 2), sig_marker) else "NA"
+ 
+   # Return results
+   data.frame(
+     Variable = var,
+     Control = control_mean,
+     Treatment = treat_mean,
+     F_test = f_test
+   )
+ })
> 
> # --- INTENT: Combine the list of individual variable results into a single data frame
> results_df <- bind_rows(results)
> 
> # --- INTENT: Calculate the difference in percentage points between Treatment and Control
> results_df <- results_df %>% 
+   mutate(
+     Difference_pp = (Treatment - Control) * 100
+   )
> 
> # --- INTENT: Define user-friendly labels for the business outcome variables to be used in the final table
> # Define nice variable labels
> var_labels <- c(
+   "bus.location" = "Selected location",
+   "bus.open" = "Opened business", 
+   "bus.purchases" = "Made purchases",
+   "bus.hire" = "Hired employees",
+   "bus.profit" = "Made profit",
+   "bus.close" = "Closed business"
+ )
> 
> # --- INTENT: Replace the technical variable names in the results data frame with the user-friendly labels
> # Replace variable names with labels
> results_df$Variable <- var_labels[results_df$Variable]
> 
> # --- INTENT: Format the calculated means as percentages and the difference to two decimal places
> results_df <- results_df %>%
+   mutate(
+     Control = scales::percent(Control, accuracy = 0.01),
+     Treatment = scales::percent(Treatment, accuracy = 0.01),
+     Difference_pp = sprintf("%.2f", Difference_pp) # Format difference to 2 decimal places
+   )
> 
> # --- INTENT: Generate the final, formatted LaTeX table
> results_df %>%
+   dplyr::select(Variable, Control, Treatment, Difference_pp, F_test) %>% # Added Difference_pp
+   kbl(
+     caption = "Key Business Outcomes by Treatment Group (Wave 3)",
+     col.names = c("Outcome", "Control (%)", "Treatment (%)", "Difference (p.p.)", "F test"), # Updated "Test" to "F test"
+     align = c("l", "r", "r", "r", "l"), # Changed alignment of F test column from "r" to "l"
+     # digits = 2, # No longer needed after formatting
+     booktabs = TRUE
+   ) %>%
+   # Add styling options
+   kable_styling(latex_options = c("striped", "[H]"), 
+                 bootstrap_options = "striped") %>% 
+   # Update to use numbered footnotes
+   footnote(
+     number = c("Significance levels: *** p<0.001, ** p<0.01, * p<0.05, â€  p<0.1."), 
+     number_title = "Note: ",
+     general = c("Percentages calculated using Wave 3 inverse probability weights (wt3)."),
+     general_title = "",
+     footnote_as_chunk = TRUE,
+     escape = FALSE # Ensure LaTeX symbols render correctly
+   )
> # CHUNK: tableA1-replication
> # PURPOSE: Replicate baseline balance table comparing pre-treatment characteristics by group.
> # EXECUTION: select baseline covariates, create gtsummary table, and style with kableExtra.
> # FUN NOTE: Fingers crossed randomization held up.
> 
> # --- INTENT: Replicate Table A1 from the paper's appendix, which presents a baseline balance check. ---
> # --- EFFECT: This chunk selects baseline covariates from `mig_wide`, uses the `gtsummary` package to create a formatted summary table comparing these covariates across treatment and control groups, adds significance tests (p-values) for differences, and displays the resulting table. ---
> # message("Running Table A1 balance check...")
> 
> # --- INTENT: Ensure the `gtsummary` package is loaded for creating the balance table. ---
> # --- EFFECT: Loads the `gtsummary` package (likely already loaded in setup). ---
> # Load gtsummary explicitly if not already loaded (Moved to setup chunk)
> # library(gtsummary)
> 
> # --- INTENT: Define the specific baseline covariate variable names (columns in `mig_wide`) to be included in the balance check. ---
> # --- EFFECT: Creates a character vector `baseline_vars` containing the names of the relevant columns measured before the treatment intervention began. Comments note variables mentioned in original code/paper but not present or suitable here. ---
> # Select baseline covariates and treatment indicator from mig_wide
> # Note: mg.asp (aspiration) is not measured at baseline
> # Note: gvt.rg.cmpl & bs.supp are not in mig_wide baseline
> baseline_vars <- c(
+   "age", "educ", "male", "Region", 
+   "bs.exp.now", "bs.exp.past", "Score", "lost.job", "job_w1", 
+   "mg.pln_w1" # Migration PLANS at baseline
+ )
> 
> # --- INTENT: Create a subset of the data containing only the treatment indicator and the selected baseline covariates. ---
> # --- EFFECT: Uses `dplyr::select` to create a new tibble `balance_data` containing only the `treat.dum` column and all columns listed in the `baseline_vars` vector from the `mig_wide` dataset. ---
> balance_data <- mig_wide %>% 
+   dplyr::select(treat.dum, all_of(baseline_vars))
> 
> # --- INTENT: Generate the formatted balance table using `gtsummary::tbl_summary`. ---
> # --- EFFECT: Pipes the `balance_data` into `tbl_summary` and applies several customizations:
> #           - `by = treat.dum`: Specifies that summary statistics should be calculated separately for each level of the treatment dummy (Control vs. Treatment).
> #           - `label = list(...)`: Provides user-friendly display labels for the variables, overriding the technical column names.
> #           - `statistic = list(...)`: Defines how statistics should be displayed. Continuous variables show mean (sd), categorical variables show count (percentage).
> #           - `digits = list(...)`: Controls the number of digits displayed for continuous means/SDs and categorical percentages.
> #           - `missing = "no"`: Excludes rows showing counts of missing values.
> #           Then, pipes the `tbl_summary` object through further modifications:
> #           - `add_p(test = list(...))`: Calculates and adds p-values comparing the groups. Specifies appropriate statistical tests (`t.test` for continuous, `chisq.test` for categorical) for different variable types. `pvalue_fun` formats the p-value display.
> #           - `add_overall()`: Adds an extra column showing summary statistics for the overall sample (both groups combined).
> #           - `modify_header()`: Customizes the header for the variable label column.
> #           - `modify_spanning_header()`: Creates a spanning header "Treatment Status" over the Control and Treatment columns.
> #           - `modify_caption()`: Sets the main caption for the table.
> #           - `bold_labels()`: Bolds the variable labels in the first column.
> #           Assigns the final formatted table object to `balance_table`. ---
> # Create the balance table using gtsummary
> # - 'by = treat.dum' creates columns for Control (0) and Treatment (1)
> # - 'label =' provides nicer names for variables
> # - 'add_p()' calculates p-values to test for differences (e.g., t-test, chi-sq test)
> # - We might need to adjust statistic types (e.g., show mean/sd for continuous)
> 
> balance_table <- balance_data %>% 
+   tbl_summary(
+     by = treat.dum, 
+     label = list( # Nicer labels for display
+       age ~ "Age",
+       educ ~ "Education Level",
+       male ~ "Male",
+       Region ~ "Region",
+       bs.exp.now ~ "Currently has business",
+       bs.exp.past ~ "Had business in past",
+       Score ~ "Eligibility Score",
+       lost.job ~ "COVID Job Loss Status",
+       job_w1 ~ "Has Job (Baseline)",
+       mg.pln_w1 ~ "Migration Plans (Baseline, 1-3)"
+     ),
+     statistic = list( # Specify stats: mean (sd) for continuous
+       all_continuous() ~ "{mean} ({sd})",
+       all_categorical() ~ "{n} ({p}%)"
+     ),
+     digits = list( # Control digits
+       all_continuous() ~ 2,
+       all_categorical() ~ 1
+     ),
+     missing = "no" # Don't show missing counts separately
+   ) %>% 
+   # Specify correct tests for each variable type
+   add_p(test = list( 
+       # Continuous variables
+       all_continuous() ~ "t.test", 
+       # Multi-category variables
+       c(Region, educ, lost.job, mg.pln_w1) ~ "chisq.test",
+       # Binary categorical variables (coded 0/1 or similar) 
+       # We can list them explicitly or let tbl_summary guess default if appropriate
+       # Explicitly: c(male, bs.exp.now, bs.exp.past, job_w1) ~ "prop.test"
+       # Or rely on default for binary after handling continuous/multi-category above
+       # Let's try relying on default for remaining (binary) after specifying others
+       # If errors persist for binary vars, specify "prop.test" or "chisq.test" explicitly
+       all_categorical() ~ "chisq.test" # Fallback to chisq for any remaining categoricals
+     ),
+     pvalue_fun = ~style_pvalue(.x, digits = 3)
+   ) %>% 
+   add_overall() %>% # Add a column for overall statistics
+   modify_header(label ~ "**Characteristic**") %>% 
+   modify_spanning_header(c("stat_1", "stat_2") ~ "**Treatment Status**") %>% 
+   modify_caption("**Baseline Characteristics by Treatment Status**") %>% 
+   bold_labels()
> 
> # --- INTENT: Convert the gtsummary table to kableExtra and apply styling. ---
> # --- EFFECT: Uses as_kable_extra() to convert the gtsummary object, then kable_styling() adds booktabs style and striped rows for LaTeX output. ---
> # Display the table using kableExtra for styling
> balance_table %>%
+   as_kable_extra(booktabs = TRUE) %>% # Convert and enable booktabs
+   kable_styling(latex_options = c("striped", "[H]"), # Add striping & [H] for LaTeX
+                 bootstrap_options = "striped", # Add striping for HTML (consistency)
+                 full_width = FALSE) # Prevent table stretching full page width
> 
> # --- OLD WAY: Display the table (removed) ---
> # balance_table
> # CHUNK: table2-models
> # PURPOSE: Estimate OLS models for short- and long-term treatment effects.
> # EXECUTION: Fit difference-in-means and DiD specifications (weighted and unweighted).
> # FUN NOTE: OLS never looked so good.
> 
> # Load necessary libraries (dplyr likely loaded in setup)
> # library(dplyr) 
> 
> # Ensure mig_long is prepared (from setup chunk or previous steps)
> if (!exists("mig_long")) {
+   stop("Error in table2-models: mig_long data frame not found.")
+ }
> 
> # --- Estimate Models (using nonmissing2/nonmissing3 for exact replication) --- 
> 
> # Model 1: Difference-in-Means (Wave 2 only)
> asp.w2 <- lm(mg.asp ~ treat.dum + factor(Region), 
+              data=subset(mig_long, wave==2 & nonmissing2), 
+              weights=wt2)
> 
> # Model 2: Difference-in-Differences (Baseline vs Wave 2, Weighted)
> asp.did.w2 <- lm(mg.asp ~ treat.dum*post.dum + factor(Region), 
+                  data=subset(mig_long, wave!=3 & nonmissing2), 
+                  weights=wt2)
> 
> # Model 3: Difference-in-Means (Wave 3 only)
> asp.w3 <- lm(mg.asp ~ treat.dum + factor(Region), 
+              data=subset(mig_long, wave==3 & nonmissing3), 
+              weights=wt3)
> 
> # Model 4: Difference-in-Differences (Baseline vs Wave 3, Weighted)
> asp.did.w3 <- lm(mg.asp ~ treat.dum*post.dum + factor(Region), 
+                  data=subset(mig_long, wave!=2 & nonmissing3), 
+                  weights=wt3)
> 
> # Model 2 (Unweighted): Difference-in-Differences (Baseline vs Wave 2)
> asp.did.w2_unw <- lm(mg.asp ~ treat.dum*post.dum + factor(Region), 
+                      data=subset(mig_long, wave!=3 & nonmissing2))
> 
> # Model 4 (Unweighted): Difference-in-Differences (Baseline vs Wave 3)
> asp.did.w3_unw <- lm(mg.asp ~ treat.dum*post.dum + factor(Region), 
+                      data=subset(mig_long, wave!=2 & nonmissing3))
> # CHUNK: table2a-replication
> # PURPOSE: Create Table 2a displaying Wave 2 models with Hochberg-adjusted p-values.
> # EXECUTION: Adjust p-values, prepare model list, and render using modelsummary.
> # FUN NOTE: P-values be stepping down like I stepped up my coffee intake.
> 
> # Load modelsummary (likely loaded in setup)
> # library(modelsummary)
> # library(tibble)
> 
> # --- Apply Hochberg step-down correction ---
> # Create a function to apply Hochberg correction to a model's p-values
> hochberg_correction <- function(model) {
+   # Extract coefficient table
+   coef_table <- summary(model)$coefficients
+   
+   # Create a copy of the model
+   adjusted_model <- model
+   
+   # Apply Hochberg correction to all p-values
+   p_values <- coef_table[, "Pr(>|t|)"]
+   adjusted_p <- p.adjust(p_values, method = "hochberg")
+   
+   # Replace standard errors with adjusted p-values for display purposes
+   # This is a hack since modelsummary doesn't easily allow showing p-values directly
+   corrected_table <- coef_table
+   corrected_table[, "Std. Error"] <- adjusted_p
+   
+   # Store the adjusted coefficient table in the model
+   attr(adjusted_model, "hochberg_coef") <- corrected_table
+   
+   # Return the modified model
+   return(adjusted_model)
+ }
> 
> # Custom tidy method for models with Hochberg correction
> tidy.hochberg <- function(model, ...) {
+   corrected_table <- attr(model, "hochberg_coef")
+   result <- data.frame(
+     term = rownames(corrected_table),
+     estimate = corrected_table[, "Estimate"],
+     std.error = corrected_table[, "Std. Error"],  # These are actually the adjusted p-values
+     statistic = corrected_table[, "t value"],
+     p.value = corrected_table[, "Std. Error"]  # Use adjusted p-values for significance stars
+   )
+   return(result)
+ }
> 
> # Apply Hochberg correction to the weighted DiD model
> asp.did.w2_hochberg <- hochberg_correction(asp.did.w2)
> class(asp.did.w2_hochberg) <- c("hochberg", class(asp.did.w2))
> 
> # --- Prepare inputs for Table 2a --- 
> models_t2a <- list(
+   `W2 Mean` = asp.w2,
+   `W2 DiD Wgt` = asp.did.w2,
+   `W2 DiD Hoch.` = asp.did.w2_hochberg,
+   `W2 DiD Unw` = asp.did.w2_unw
+ )
> 
> stars_map <- c("â€ " = 0.1, "*" = 0.05, "**" = 0.01, "***" = 0.001)
> 
> add_rows_t2a <- tibble::tribble(
+   ~term,           ~`W2 Mean`, ~`W2 DiD Wgt`, ~`W2 DiD Hoch.`, ~`W2 DiD Unw`,
+   "Fixed effects", "Yes", "Yes", "Yes", "Yes",
+   "IPW Weighted",  "Yes", "Yes", "Yes", "No",
+   "Multiple test corr.", "No", "No", "Yes", "No"
+ )
> 
> # Custom function for modelsummary to handle our hochberg models
> glance.hochberg <- function(x, ...) {
+   # Extract the original model's glance information
+   original_glance <- glance(x$model)
+   # Return the same glance information
+   return(original_glance)
+ }
> 
> # --- Generate Table 2a --- 
> modelsummary(models_t2a,
+              output = "kableExtra", 
+              fmt = 3, 
+              stars = stars_map, 
+              coef_omit = "Region", 
+              coef_rename = c("treat.dum" = "Treat assignment", 
+                            "post.dum" = "Post-treatment", 
+                            "treat.dum:post.dum" = "Treat x Post",
+                            "(Intercept)" = "Constant"), 
+              gof_map = list(
+                list("raw" = "nobs", "clean" = "Observations", "fmt" = 0),
+                list("raw" = "r.squared", "clean" = "R$^{2}$", "fmt" = 3),
+                list("raw" = "adj.r.squared", "clean" = "Adj. R$^{2}$", "fmt" = 3)
+              ),
+              add_rows = add_rows_t2a,
+              notes = list("SEs in parentheses; Hochberg column shows adjusted p-values",
+                         "Models: (1) weighted, (2a) weighted DiD, (2b) with multiple testing correction, (2c) unweighted",
+                         "All include regional fixed effects"),
+              title = "Effect of Program on Migration Aspiration (Wave 2 Models)",
+              escape = FALSE
+              ) %>%
+   kable_styling(latex_options = c("striped", "scale_down", "HOLD_position"), 
+                 bootstrap_options = "striped",
+                 position = "center",
+                 font_size = 9,
+                 full_width = FALSE)
> # CHUNK: table2b-replication
> # PURPOSE: Create Table 2b showing Wave 3 models with p-values after Hochberg correction.
> # EXECUTION: Apply correction, assemble model list, and render table with styling.
> # FUN NOTE: Remember when I thought replication was easy? Good times.
> 
> # Load modelsummary (likely loaded in setup)
> # library(modelsummary)
> # library(tibble)
> 
> # --- Apply Hochberg step-down correction for Wave 3 ---
> # Use the same hochberg_correction function from the previous chunk
> 
> # Apply Hochberg correction to the Wave 3 weighted DiD model
> asp.did.w3_hochberg <- hochberg_correction(asp.did.w3)
> class(asp.did.w3_hochberg) <- c("hochberg", class(asp.did.w3))
> 
> # --- Prepare inputs for Table 2b --- 
> models_t2b <- list(
+   `W3 Mean` = asp.w3,
+   `W3 DiD Wgt` = asp.did.w3,
+   `W3 DiD Hoch.` = asp.did.w3_hochberg,
+   `W3 DiD Unw` = asp.did.w3_unw
+ )
> 
> # Define stars map again (or ensure it's global)
> stars_map <- c("â€ " = 0.1, "*" = 0.05, "**" = 0.01, "***" = 0.001)
> 
> add_rows_t2b <- tibble::tribble(
+   ~term,           ~`W3 Mean`, ~`W3 DiD Wgt`, ~`W3 DiD Hoch.`, ~`W3 DiD Unw`,
+   "Fixed effects", "Yes", "Yes", "Yes", "Yes",
+   "IPW Weighted",  "Yes", "Yes", "Yes", "No",
+   "Multiple test corr.", "No", "No", "Yes", "No"
+ )
> 
> # --- Generate Table 2b --- 
> modelsummary(models_t2b,
+              output = "kableExtra", 
+              fmt = 3, 
+              stars = stars_map, 
+              coef_omit = "Region", 
+              coef_rename = c("treat.dum" = "Treat assignment", 
+                            "post.dum" = "Post-treatment", 
+                            "treat.dum:post.dum" = "Treat x Post",
+                            "(Intercept)" = "Constant"), 
+              gof_map = list(
+                list("raw" = "nobs", "clean" = "Observations", "fmt" = 0),
+                list("raw" = "r.squared", "clean" = "R$^{2}$", "fmt" = 3),
+                list("raw" = "adj.r.squared", "clean" = "Adj. R$^{2}$", "fmt" = 3)
+              ),
+              add_rows = add_rows_t2b,
+              notes = list("SEs in parentheses; Hochberg column shows adjusted p-values",
+                         "Models: (3) weighted, (4a) weighted DiD, (4b) with multiple testing correction, (4c) unweighted",
+                         "All include regional fixed effects"),
+              title = "Effect of Program on Migration Aspiration (Wave 3 Models)",
+              escape = FALSE
+              ) %>%
+   kable_styling(latex_options = c("striped", "scale_down", "HOLD_position"), 
+                 bootstrap_options = "striped",
+                 position = "center",
+                 font_size = 9,
+                 full_width = FALSE)
> # Extract key coefficients and format them for display
> wgt_coef <- round(coef(summary(asp.did.w2))["treat.dum:post.dum", "Estimate"], 3)
> wgt_se <- round(coef(summary(asp.did.w2))["treat.dum:post.dum", "Std. Error"], 3)
> unwgt_coef <- round(coef(summary(asp.did.w2_unw))["treat.dum:post.dum", "Estimate"], 3)
> unwgt_se <- round(coef(summary(asp.did.w2_unw))["treat.dum:post.dum", "Std. Error"], 3)
> orig_coef <- -0.301  # From Table 2 in the original paper (basepaper.md, line ~876)
> 
> cat(paste0(
+   "\n## Table 2 Replication Approach\n\n",
+   "We modified the original Table 2 presentation by separating Wave 2 and Wave 3 results into Tables 2a and 2b for clearer interpretation. ",
+   "The original paper presented all four models in a single table, making it harder to distinguish between short-term and longer-term effects.\n\n",
+   "We also added unweighted difference-in-differences (DiD) models alongside the original weighted specifications. ",
+   "This addition serves as a robustness check to determine whether the results depend on the inverse probability weighting (IPW) scheme. ",
+   "Weighted models can be sensitive to how weights are constructed, especially in panel data where dropout patterns might differ between treatment and control groups.\n\n",
+   "Notably, our weighted and unweighted Wave 2 DiD models show virtually identical results (Treat x Post coefficient: Weighted = ", wgt_coef, ", Unweighted = ", unwgt_coef, ", ",
+   "with nearly identical standard errors: Weighted SE = ", wgt_se, ", Unweighted SE = ", unwgt_se, "). ",
+   "This similarity suggests that attrition in the study was essentially random with respect to treatment status and outcomes, meaning the weighting procedure had negligible impact. ",
+   "This strengthens confidence in the robustness of the findings, as they're consistent regardless of whether weights are applied. ",
+   "Both estimates now closely match the original paper's coefficient (", orig_coef, ").\n\n",
+   "This similarity raises the question of whether the original authors' use of inverse probability weighting added unnecessary methodological complexity, given its minimal effect on results. ",
+   "Our robustness check strengthens confidence in the paper's conclusions about program effects on migration aspirations, as the treatment effects aren't merely artifacts of the weighting method.\n"
+ ))
> # --- INTENT: Replicate Figure 2 from the paper (Coefficient Plot) ---
> # --- EFFECT: This entire chunk calculates treatment effects on mediator variables and generates a ggplot object replicating the paper's Figure 2 visualization. ---
> # message("Running Figure 2 replication...")
> 
> # Ensure necessary libraries are loaded (broom, ggplot2, dplyr, RColorBrewer) - Loaded in setup
> 
> # Define position dodging for ggplot elements to prevent overlap.
> pd <- position_dodge(width = 0.3) # Define position dodge
> 
> # Use a professional color palette - blue/orange contrast (colorblind-friendly)
> color_palette <- c("#2b8cbe", "#e66101") # Professional blue and orange
> 
> # Define the specific mediator variable names as they appear in the `mig_wide` dataset for Wave 2 and Wave 3.
> meds_w2 <- c("self.suff_w2", "efficacy_w2", "placeat_w2") 
> meds_w3 <- c("self.suff_w3", "efficacy_w3", "placeat_w3")
> 
> # Define a reusable function to perform the core regression analysis for a single mediator variable.
> run_mediator_lm <- function(mediator, wave_label, weight_var, nonmissing_var, data) {
+   # Create the formula for the linear model dynamically based on the mediator name.
+   formula_str <- paste(mediator, "~ treat.dum + factor(Region)")
+   
+   # Filter the dataset using the proper nonmissing indicator variable to match original script
+   subset_data <- data %>% filter(.data[[nonmissing_var]] == TRUE)
+   
+   # Add robustness checks to prevent errors if mediator variable is missing or has insufficient data after filtering.
+   if (!mediator %in% names(subset_data) || all(is.na(subset_data[[mediator]]))) {
+     warning(paste("Mediator variable", mediator, "not found or all NA in subset for", nonmissing_var))
+     return(NULL)
+   }
+   # Further filter out rows where the *mediator* variable itself is NA to ensure the model runs correctly.
+   subset_data <- subset_data %>% filter(!is.na(.data[[mediator]]))
+   # Add another robustness check for a minimum number of observations required to run the regression reliably.
+   if (nrow(subset_data) < 10) { # Added check for minimum observations
+      warning(paste("Insufficient non-NA observations for", mediator, "with", nonmissing_var))
+      return(NULL)
+   }
+ 
+   # Run the weighted linear regression as specified by the paper's analysis for Figure 2.
+   model <- lm(as.formula(formula_str), data = subset_data, weights = subset_data[[weight_var]])
+   
+   # Extract the key results (coefficient, standard error, confidence interval) for the treatment effect ('treat.dum') in a standardized format.
+   tidy_model <- broom::tidy(model, conf.int = TRUE) %>%
+     filter(term == "treat.dum") %>%
+     mutate(mediator_base = gsub("_w[23]$", "", mediator), # Get base name (e.g., "self.suff")
+            wave = !!wave_label) # Use descriptive wave label
+   # Return the processed results from the function.
+   return(tidy_model)
+ }
> 
> # Apply the `run_mediator_lm` function to each Wave 2 mediator variable.
> results_w2 <- lapply(meds_w2, run_mediator_lm, 
+                     wave_label = "Short-term (Wave 2)", 
+                     weight_var = "wt2", 
+                     nonmissing_var = "nonmissing2",
+                     data = mig_wide) %>%
+   bind_rows() %>% 
+   filter(!is.null(.)) # Remove NULLs if any model failed
> 
> # Apply the `run_mediator_lm` function to each Wave 3 mediator variable.
> results_w3 <- lapply(meds_w3, run_mediator_lm, 
+                     wave_label = "Long-term (Wave 3)", 
+                     weight_var = "wt3",
+                     nonmissing_var = "nonmissing3",
+                     data = mig_wide) %>%
+   bind_rows() %>%
+   filter(!is.null(.)) # Remove NULLs if any model failed
> 
> # Combine the results from both waves into a single data frame for plotting.
> all_results <- bind_rows(results_w2, results_w3)
> 
> # Create a mapping from the base mediator variable names (e.g., "self.suff") to more descriptive labels suitable for the plot's y-axis, including line breaks.
> mediator_labels <- c(
+   "self.suff" = "Self-sufficiency",
+   "efficacy" = "Personal efficacy",
+   "placeat" = "Financial success at home"
+ )
> 
> # Define the desired order of mediator labels on the y-axis, matching the original paper's Figure 2 presentation (top-to-bottom).
> ordered_labels <- c(
+     "Financial success at home", 
+     "Personal efficacy",
+     "Self-sufficiency"
+ )
> 
> # Prepare the final data frame for plotting by adding the display labels and ensuring they are ordered correctly.
> plot_data <- all_results %>%
+   mutate(mediator_label = factor(mediator_labels[mediator_base], levels = ordered_labels)) %>% # Use ordered levels
+   filter(!is.na(mediator_label)) # Ensure only valid mediators are plotted
> 
> # Create the plot - simpler version with minimal theming
> fig2_plot <- ggplot(plot_data, aes(x = estimate, y = mediator_label, color = wave, shape = wave)) +
+   # Use clean theme with white background
+   theme_bw() +
+   
+   # Reference line at zero
+   geom_vline(xintercept = 0, linetype = "dashed", color = "grey60", linewidth = 0.7) +
+   
+   # Error bars and points
+   geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2, position = pd, linewidth = 0.8) +
+   geom_point(position = pd, size = 3, stroke = 1.2) +
+   
+   # Set colors and shapes
+   scale_color_manual(values = color_palette, name = "Time Point") +
+   scale_shape_manual(values = c("Short-term (Wave 2)" = 16, "Long-term (Wave 3)" = 1), name = "Time Point") +
+   
+   # Simple labels
+   labs(
+     title = "Figure 2: Treatment Effects on Psychological Mediators",
+     subtitle = "Difference-in-means estimates (Treatment - Control) with 95% CIs",
+     x = "Difference in Means",
+     y = NULL
+   ) +
+   
+   # Clean theme with minimal styling
+   theme(
+     # Text elements
+     plot.title = element_text(face = "bold", size = 12, hjust = 0.5),
+     plot.subtitle = element_text(size = 10, hjust = 0.5),
+     axis.title.x = element_text(size = 10),
+     axis.text.y = element_text(hjust = 1, face = "bold"),
+     
+     # Legend
+     legend.position = "top",
+     legend.box = "horizontal",
+     legend.title = element_text(face = "bold"),
+     
+     # Panel
+     panel.grid.major.y = element_blank(),
+     panel.grid.minor = element_blank()
+   ) +
+   
+   # X-axis limits
+   coord_cartesian(xlim = c(-0.3, 0.8))
> 
> # Save and display the plot
> ggsave("figure2.pdf", plot = fig2_plot, width = 6.5, height = 4.5, device = "pdf", dpi = 300)
> print(fig2_plot)
> # --- INTENT: Replicate Figure 3 using iterative mediate() calls ---
> # message("Figure 3 - Mediation time! Running 1000 simulations... go grab a coffee, this might take a while. Or maybe it'll crash. â˜• / ðŸ’¥")
> message("Running Figure 3 mediation analysis...")
> 
> # Load necessary libraries (Moved to setup chunk)
> # library(mediation)
> # library(Hmisc)
> # library(tidyverse)
> # library(RColorBrewer)
> 
> # Define base mediator names (without wave suffix)
> mediators_base <- Cs(self.suff, efficacy, placeat)
> 
> # Define confounding variables (covariates)
> confounders <- Cs(age, educ, male, bs.exp.now, bs.exp.past, Score)
> 
> # Treatment variable
> treatment <- "treat.dum"
> 
> # Create consistent color palette with Figure 2
> color_palette <- c("#2b8cbe", "#e66101") # Professional blue and orange
> pd <- position_dodge(width = 0.3) 
> 
> # Initialize an empty list to store results from each mediate() call
> results_list <- list()
> 
> # --- Loop through each mediator and each wave --- 
> for (med_base in mediators_base) {
+   for (wave_num in c(2, 3)) {
+     
+     # Define wave-specific variables
+     wave_label <- paste0("Wave ", wave_num)
+     outcome_var <- paste0("mg.asp_w", wave_num)
+     mediator_var <- paste0(med_base, "_w", wave_num)
+     weight_var <- paste0("wt", wave_num)
+     nonmissing_var <- paste0("nonmissing", wave_num)
+     wave_time_label <- ifelse(wave_num == 2, "Short-term (Wave 2)", "Long-term (Wave 3)")
+ 
+     # Check if required base data exists
+     if (!exists("mig_wide")) {
+       stop("Error: mig_wide not found. Please ensure setup chunk has run.")
+     }
+     
+     # Prepare data for this specific mediator/wave using proper nonmissing filter
+     data_wave_df <- as.data.frame(subset(mig_wide, mig_wide[[nonmissing_var]] == TRUE))
+     required_cols <- c(outcome_var, mediator_var, treatment, confounders, weight_var)
+     missing_cols <- setdiff(required_cols, names(data_wave_df))
+     
+     if (length(missing_cols) > 0) {
+       next # Skip to next iteration
+     }
+     
+     # Filter for complete cases
+     complete_cases_data <- data_wave_df[complete.cases(data_wave_df[, required_cols]), ]
+     n_complete <- nrow(complete_cases_data)
+     
+     # Skip if insufficient data
+     if (n_complete < 30) {
+       next 
+     }
+     
+     # Fit Mediator Model
+     med_formula_str <- paste(mediator_var, "~", treatment, "+", paste(confounders, collapse=" + "))
+     med_model <- tryCatch({
+       lm(as.formula(med_formula_str), 
+          data = complete_cases_data, 
+          weights = complete_cases_data[[weight_var]])
+     }, error = function(e) {
+       return(NULL)
+     })
+     
+     if (is.null(med_model)) next
+ 
+     # Fit Outcome Model
+     out_formula_str <- paste(outcome_var, "~", treatment, "*", mediator_var, "+", paste(confounders, collapse=" + "))
+     out_model <- tryCatch({
+       lm(as.formula(out_formula_str), 
+          data = complete_cases_data, 
+          weights = complete_cases_data[[weight_var]])
+     }, error = function(e) {
+       return(NULL)
+     })
+     
+     if (is.null(out_model)) next
+ 
+     # Run Mediation
+     mediation_result <- tryCatch({
+       mediate(med_model, out_model, 
+               treat = treatment, 
+               mediator = mediator_var, 
+               sims = 1000)
+     }, error = function(e) {
+       return(NULL)
+     })
+ 
+     # Extract Results if successful
+     if (!is.null(mediation_result)) {
+       summary_med <- summary(mediation_result)
+       
+       # Extract ACME values
+       if (!is.null(summary_med$d.avg) && length(summary_med$d.avg) == 1 && is.numeric(summary_med$d.avg) &&
+           !is.null(summary_med$d.avg.ci) && length(summary_med$d.avg.ci) == 2 && is.numeric(summary_med$d.avg.ci)) {
+           
+           estimate_val <- summary_med$d.avg
+           conf_low_val <- summary_med$d.avg.ci[1]
+           conf_high_val <- summary_med$d.avg.ci[2]
+           
+           if (!is.null(estimate_val) && !is.null(conf_low_val) && !is.null(conf_high_val)) {
+               results_list[[length(results_list) + 1]] <- data.frame(
+                 mediator = med_base,
+                 estimate = estimate_val, 
+                 conf_low = conf_low_val,
+                 conf_high = conf_high_val,
+                 wave = wave_time_label,
+                 stringsAsFactors = FALSE
+               )
+            }
+       }
+     }
+   }
+ }
> 
> # --- Combine and Plot Results --- 
> if (length(results_list) > 0) {
+   all_med_results <- bind_rows(results_list)
+   
+   # Create mapping from variable names to nicer labels
+   mediator_labels <- c(
+     "self.suff" = "Self-sufficiency",
+     "efficacy" = "Personal efficacy",
+     "placeat" = "Financial success at home"
+   )
+   
+   # Order for y-axis (from top to bottom)
+   ordered_labels <- c(
+     "Financial success at home",
+     "Personal efficacy",
+     "Self-sufficiency"
+   )
+   
+   # Process results for plotting
+   plot_data <- all_med_results %>%
+     mutate(
+       mediator_label = factor(mediator_labels[mediator], levels = ordered_labels) 
+     ) %>% 
+     filter(!is.na(mediator_label))
+   
+   if (nrow(plot_data) > 0) {
+       # Create the plot with the same clean style as Figure 2
+       fig3_plot <- ggplot(plot_data, 
+                          aes(x = estimate, y = mediator_label, 
+                              color = wave, shape = wave)) +
+         # Use basic theme
+         theme_bw() +
+         
+         # Reference line at zero
+         geom_vline(xintercept = 0, linetype = "dashed", color = "grey60", linewidth = 0.7) +
+         
+         # Error bars and points
+         geom_errorbarh(aes(xmin = conf_low, xmax = conf_high), 
+                       height = 0.2, position = pd, linewidth = 0.8) +
+         geom_point(position = pd, size = 3, stroke = 1.2) +
+         
+         # Colors and shapes
+         scale_color_manual(values = color_palette, name = "Time Point") +
+         scale_shape_manual(values = c("Short-term (Wave 2)" = 16, "Long-term (Wave 3)" = 1), 
+                           name = "Time Point") +
+         
+         # Simple labels
+         labs(
+           title = "Figure 3: Average Causal Mediation Effects (ACME)",
+           subtitle = "Iterative 'mediate' estimates (1000 simulations) with 95% CIs",
+           x = "Average Causal Mediation Effect (ACME)",
+           y = NULL
+         ) +
+         
+         # Minimal theme
+         theme(
+           # Text elements
+           plot.title = element_text(face = "bold", size = 12, hjust = 0.5),
+           plot.subtitle = element_text(size = 10, hjust = 0.5),
+           axis.title.x = element_text(size = 10),
+           axis.text.y = element_text(hjust = 1, face = "bold"),
+           
+           # Legend
+           legend.position = "top",
+           legend.box = "horizontal",
+           legend.title = element_text(face = "bold"),
+           
+           # Panel
+           panel.grid.major.y = element_blank(),
+           panel.grid.minor = element_blank()
+         ) +
+         
+         # Appropriate x-axis limits
+         coord_cartesian(xlim = c(-0.15, 0.15))
+       
+       # Save and display the plot
+       ggsave("figure3.pdf", plot = fig3_plot, width = 6.5, height = 4.5, device = "pdf", dpi = 300)
+       print(fig3_plot)
+   } else {
+       # message("Plotting skipped: No valid data after processing results.") # Commented out
+   }
+ } else {
+   # message("Plotting skipped: No successful mediation results were obtained from any mediator/wave combination.") # Commented out
+ }
> # --- INTENT: Estimate DiD models replacing region FE with baseline covariates. ---
> # --- EFFECT: Runs four lm() models using the original mig_long dataset. ---
> message("Running Robustness Check 1: Covariate Adjustment Models...")
> 
> # Define baseline covariates (already in mig_long)
> baseline_controls <- c("age", "educ", "male", "Score")
> baseline_formula_part <- paste(baseline_controls, collapse = " + ")
> 
> # Ensure mig_long exists before proceeding
> if (!exists("mig_long")) {
+   stop("Error in robust-covariate-adj-models: mig_long data frame not found.")
+ }
> 
> # Model R1a: DiD (Baseline vs Wave 2, Weighted, Covariates)
> asp.did.w2.cov <- lm(as.formula(paste("mg.asp ~ treat.dum*post.dum +", baseline_formula_part)),
+                      data=subset(mig_long, wave!=3 & nonmissing2), # Use mig_long
+                      weights=wt2)
> # print(summary(asp.did.w2.cov)) # DEBUG PRINT REMOVED
> 
> # Model R1b: DiD (Baseline vs Wave 2, Unweighted, Covariates)
> asp.did.w2.cov_unw <- lm(as.formula(paste("mg.asp ~ treat.dum*post.dum +", baseline_formula_part)),
+                          data=subset(mig_long, wave!=3 & nonmissing2)) # Use mig_long
> # print(summary(asp.did.w2.cov_unw)) # DEBUG PRINT REMOVED
> 
> # Model R1c: DiD (Baseline vs Wave 3, Weighted, Covariates)
> asp.did.w3.cov <- lm(as.formula(paste("mg.asp ~ treat.dum*post.dum +", baseline_formula_part)),
+                      data=subset(mig_long, wave!=2 & nonmissing3), # Use mig_long
+                      weights=wt3)
> # print(summary(asp.did.w3.cov)) # DEBUG PRINT REMOVED
> 
> # Model R1d: DiD (Baseline vs Wave 3, Unweighted, Covariates)
> asp.did.w3.cov_unw <- lm(as.formula(paste("mg.asp ~ treat.dum*post.dum +", baseline_formula_part)),
+                          data=subset(mig_long, wave!=2 & nonmissing3)) # Use mig_long
> # print(summary(asp.did.w3.cov_unw)) # DEBUG PRINT REMOVED
> 
> message("Covariate adjustment models estimated.")
> # --- INTENT: Display the results of the covariate adjustment models. ---
> # --- EFFECT: Uses modelsummary to create a table comparing original DiD models with covariate-adjusted models. ---
> message("Generating table for Robustness Check 1...")
Generating table for Robustness Check 1...
> 
> models_cov_adj <- list()
> # Add models only if they exist (were estimated successfully previously)
> if (exists("asp.did.w2") && !is.null(asp.did.w2)) models_cov_adj[["(2a) W2 DiD Wgt (FE)"]] <- asp.did.w2
> if (exists("asp.did.w2.cov") && !is.null(asp.did.w2.cov)) models_cov_adj[["(R1a) W2 DiD Wgt (Cov)"]] <- asp.did.w2.cov
> if (exists("asp.did.w2_unw") && !is.null(asp.did.w2_unw)) models_cov_adj[["(2b) W2 DiD Unw (FE)"]] <- asp.did.w2_unw
> if (exists("asp.did.w2.cov_unw") && !is.null(asp.did.w2.cov_unw)) models_cov_adj[["(R1b) W2 DiD Unw (Cov)"]] <- asp.did.w2.cov_unw
> if (exists("asp.did.w3") && !is.null(asp.did.w3)) models_cov_adj[["(4a) W3 DiD Wgt (FE)"]] <- asp.did.w3
> if (exists("asp.did.w3.cov") && !is.null(asp.did.w3.cov)) models_cov_adj[["(R1c) W3 DiD Wgt (Cov)"]] <- asp.did.w3.cov
> if (exists("asp.did.w3_unw") && !is.null(asp.did.w3_unw)) models_cov_adj[["(4b) W3 DiD Unw (FE)"]] <- asp.did.w3_unw
> if (exists("asp.did.w3.cov_unw") && !is.null(asp.did.w3.cov_unw)) models_cov_adj[["(R1d) W3 DiD Unw (Cov)"]] <- asp.did.w3.cov_unw
> 
> # --- DEBUG --- 
> # print(paste("Number of models found for Covariate Adjustment table:", length(models_cov_adj))) # EH: REMOVED
> # --- END DEBUG ---
> 
> if (length(models_cov_adj) > 0) {
+   coef_map_cov <- c("treat.dum" = "Treat assignment",
+                     "post.dum" = "Post-treatment",
+                     "treat.dum:post.dum" = "Treat x Post",
+                     "age" = "Age",
+                     "educ" = "Education",
+                     "male" = "Male",
+                     "Score" = "Score",
+                     "(Intercept)" = "Constant")
+ 
+   gof_map_cov <- list(
+       list("raw" = "nobs", "clean" = "Observations", "fmt" = 0),
+       list("raw" = "r.squared", "clean" = "R$^{2}$", "fmt" = 3),
+       list("raw" = "adj.r.squared", "clean" = "Adj. R$^{2}$", "fmt" = 3)
+   )
+ 
+   modelsummary(models_cov_adj,
+                output = "kableExtra",
+                fmt = 3,
+                stars = c("â€ " = 0.1, "*" = 0.05, "**" = 0.01, "***" = 0.001),
+                coef_map = coef_map_cov,
+                coef_omit = "^factor\\(Region\\)|^\\(Intercept\\)$", # Omit Region factors and Intercept
+                gof_map = gof_map_cov,
+                notes = list("Standard errors in parentheses.",
+                           "FE models include Region Fixed Effects.",
+                           "Cov models replace Region FE with baseline Age, Education, Male, Score."),
+                title = "Robustness Check 1: Sensitivity to Covariate Adjustment vs. Region Fixed Effects",
+                escape = FALSE
+                ) %>%
+     kable_styling(latex_options = c("striped", "hold_position", "scale_down"), # Added scale_down
+                   bootstrap_options = "striped",
+                   position = "center",
+                   font_size = 9,
+                   full_width = FALSE) %>%
+     add_header_above(c(" " = 1, "Wave 2 Models" = 4, "Wave 3 Models" = 4)) %>% # Span headers
+     column_spec(1, width = "6em")
+ } else {
+   message("Skipping Covariate Adjustment table: No models available.")
+ }
> # --- INTENT: Estimate DiD models with interaction terms for HTE using mig_long. ---
> # --- EFFECT: Runs lm() models including triple interactions (treat*post*moderator). ---
> message("Running Robustness Check 4: Heterogeneity of Treatment Effects (Interactions)...")
> 
> # Ensure mig_long exists
> if (!exists("mig_long")) {
+   stop("Error in robust-hte-models: mig_long data frame not found.")
+ }
> 
> # We use the Wave 2 dataset where effects were significant
> data_w2_hte <- subset(mig_long, wave != 3 & nonmissing2) # Use mig_long
> 
> # Base formula part
> base_hte_formula <- "mg.asp ~ treat.dum * post.dum * MODERATOR + factor(Region)"
> 
> # Function to create and run HTE model
> run_hte_model <- function(moderator_var, data) {
+   if (!moderator_var %in% names(data)) {
+     message("Moderator variable '", moderator_var, "' not found. Skipping HTE for this variable.")
+     return(NULL)
+   }
+   # Check for sufficient variation in moderator
+   if (length(unique(na.omit(data[[moderator_var]]))) < 2) {
+       message("Moderator variable '", moderator_var, "' has insufficient variation. Skipping HTE.")
+       return(NULL)
+   }
+ 
+   formula_str <- gsub("MODERATOR", moderator_var, base_hte_formula)
+   model <- tryCatch({
+       lm(as.formula(formula_str), data = data, weights = wt2)
+     }, error = function(e) {
+       message("Error fitting HTE model for ", moderator_var, ": ", e$message); return(NULL)
+   })
+   # if (!is.null(model)) print(summary(model)) # DEBUG PRINT REMOVED
+   return(model)
+ }
> 
> # Estimate models for each moderator (using variables confirmed in mig_long)
> hte_male <- run_hte_model("male", data_w2_hte)
> hte_educ <- run_hte_model("educ", data_w2_hte) # Treat educ as numeric/ordinal
> hte_bs_exp <- run_hte_model("bs.exp.now", data_w2_hte)
> 
> message("HTE interaction models estimated (if data allowed).")
> # --- INTENT: Display results of the HTE models. ---
> # --- EFFECT: Uses modelsummary, focusing on the triple interaction term. ---
> message("Generating table for Robustness Check 4...")
Generating table for Robustness Check 4...
> 
> models_hte <- list()
> if (!is.null(hte_male)) models_hte[["Interact: Male"]] <- hte_male
> if (!is.null(hte_educ)) models_hte[["Interact: Education"]] <- hte_educ
> if (!is.null(hte_bs_exp)) models_hte[["Interact: Has Business"]] <- hte_bs_exp
> 
> # Add original model for reference only if it exists
> if (exists("asp.did.w2") && !is.null(asp.did.w2)) models_hte[["(2a) Original W2 DiD Wgt"]] <- asp.did.w2
> 
> if (length(models_hte) > 1) { # Check if at least one HTE model ran + original
+   coef_map_hte <- c(
+     "treat.dum:post.dum" = "Treat x Post (Main Effect)",
+     "treat.dum:post.dum:male" = "Treat x Post x Male",
+     "treat.dum:post.dum:educ" = "Treat x Post x Education",
+     "treat.dum:post.dum:bs.exp.now" = "Treat x Post x Has Business"
+   )
+ 
+   keep_pattern_hte <- "^treat\\.dum:post\\.dum" # Regex to keep key interactions
+ 
+   gof_map_hte <- list(
+     list("raw" = "nobs", "clean" = "Observations", "fmt" = 0),
+     list("raw" = "r.squared", "clean" = "R$^{2}$", "fmt" = 3)
+   )
+ 
+   modelsummary(models_hte,
+                output = "kableExtra",
+                fmt = 3,
+                stars = c("â€ " = 0.1, "*" = 0.05, "**" = 0.01, "***" = 0.001),
+                coef_map = coef_map_hte,
+                coef_matches = keep_pattern_hte,
+                gof_map = gof_map_hte,
+                notes = list("Standard errors in parentheses.",
+                           "Models are weighted Wave 2 DiD specifications with Region FE.",
+                           "Each interaction model adds a triple interaction term using baseline moderators.",
+                           "Significance on the triple interaction suggests the treatment effect differs by the moderator."),
+                title = "Robustness Check 4: Heterogeneity of Treatment Effects (Wave 2)",
+                escape = FALSE
+                ) %>%
+     kable_styling(latex_options = c("striped", "hold_position"),
+                   bootstrap_options = "striped",
+                   position = "center",
+                   font_size = 9,
+                   full_width = FALSE) %>%
+     column_spec(1, width = "12em")
+ 
+ } else {
+   message("Skipping HTE table generation as no interaction models were successfully fitted or original model missing.")
+ }
> # Load necessary packages
> library(lme4)  # For mixed effects models to calculate ICC
> 
> # Calculate ICC for Wave 2 model
> calc_icc <- function(data, formula_str) {
+   # Fit a mixed model with random intercept for region
+   formula <- as.formula(paste(formula_str, "+ (1|Region)"))
+   re_model <- lmer(formula, data=data)
+   
+   # Extract variance components
+   vc <- VarCorr(re_model)
+   region_var <- attr(vc$Region, "stddev")^2
+   residual_var <- attr(vc, "sc")^2
+   
+   # Calculate ICC
+   icc <- region_var / (region_var + residual_var)
+   
+   # Return ICC and model
+   return(list(icc = icc, model = re_model))
+ }
> 
> # Calculate ICC for both waves
> w2_icc_result <- calc_icc(
+   subset(mig_long, wave!=3 & nonmissing2),
+   "mg.asp ~ treat.dum*post.dum"
+ )
> 
> w3_icc_result <- calc_icc(
+   subset(mig_long, wave!=2 & nonmissing3),
+   "mg.asp ~ treat.dum*post.dum"
+ )
> 
> # Extract ICCs
> w2_icc <- w2_icc_result$icc
> w3_icc <- w3_icc_result$icc
> 
> # Create a table of ICCs
> icc_table <- data.frame(
+   Wave = c("Wave 2", "Wave 3"),
+   ICC = c(w2_icc, w3_icc)
+ )
> 
> # Display ICC table with improved footnote
> icc_table %>%
+   kbl(
+     caption = "Intraclass Correlation Coefficients (ICC) by Region",
+     col.names = c("Analysis", "ICC"),
+     digits = c(0, 3),
+     align = c("l", "r"),
+     booktabs = TRUE
+   ) %>%
+   kable_styling(
+     latex_options = c("striped", "HOLD_position"), 
+     bootstrap_options = "striped",
+     position = "center",
+     font_size = 9,
+     full_width = FALSE
+   ) %>%
+   footnote(
+     general = "ICC measures the proportion of variance attributable to regional clusters.",
+     footnote_as_chunk = TRUE
+   )
> # Load necessary packages
> library(sandwich)  # For robust standard errors
> library(lmtest)    # For coeftest
> 
> # Define data subsets outside the function so they're available in the global environment
> w2_data <- subset(mig_long, wave!=3 & nonmissing2)
> w3_data <- subset(mig_long, wave!=2 & nonmissing3)
> 
> # Function to create a clean table directly
> create_cluster_table <- function() {
+   # Standard results for Wave 2
+   w2_coef <- coef(summary(asp.did.w2))["treat.dum:post.dum", "Estimate"]
+   w2_se <- coef(summary(asp.did.w2))["treat.dum:post.dum", "Std. Error"]
+   w2_p <- coef(summary(asp.did.w2))["treat.dum:post.dum", "Pr(>|t|)"]
+   
+   # Clustered results for Wave 2
+   w2_cl <- coeftest(asp.did.w2, vcov = vcovCL(asp.did.w2, cluster = w2_data$Region))
+   w2_cl_se <- w2_cl["treat.dum:post.dum", "Std. Error"]
+   w2_cl_p <- w2_cl["treat.dum:post.dum", "Pr(>|t|)"]
+   
+   # Standard results for Wave 3
+   w3_coef <- coef(summary(asp.did.w3))["treat.dum:post.dum", "Estimate"]
+   w3_se <- coef(summary(asp.did.w3))["treat.dum:post.dum", "Std. Error"]
+   w3_p <- coef(summary(asp.did.w3))["treat.dum:post.dum", "Pr(>|t|)"]
+   
+   # Clustered results for Wave 3
+   w3_cl <- coeftest(asp.did.w3, vcov = vcovCL(asp.did.w3, cluster = w3_data$Region))
+   w3_cl_se <- w3_cl["treat.dum:post.dum", "Std. Error"]
+   w3_cl_p <- w3_cl["treat.dum:post.dum", "Pr(>|t|)"]
+   
+   # Create table with basic R
+   result <- data.frame(
+     Model = c("Wave 2 DiD", "Wave 3 DiD"),
+     Coef = round(c(w2_coef, w3_coef), 3),
+     Std_SE = round(c(w2_se, w3_se), 3),
+     Std_p = round(c(w2_p, w3_p), 3),
+     Clust_SE = round(c(w2_cl_se, w3_cl_se), 3),
+     Clust_p = round(c(w2_cl_p, w3_cl_p), 3),
+     SE_Ratio = round(c(w2_cl_se/w2_se, w3_cl_se/w3_se), 2)
+   )
+   
+   return(result)
+ }
> 
> # Create table and format with kable
> cluster_table <- create_cluster_table()
> 
> # Format with kable and styling matching other tables
> cluster_table %>%
+   kbl(
+     caption = "Effect of Regional Clustering on Standard Errors (DiD Treatment Effect)",
+     col.names = c("Model", "Coefficient", "Standard SE", "Standard p", 
+                  "Clustered SE", "Clustered p", "SE Ratio"),
+     digits = c(0, 3, 3, 3, 3, 3, 2),
+     align = c("l", "r", "r", "r", "r", "r", "r"),
+     booktabs = TRUE
+   ) %>%
+   kable_styling(
+     latex_options = c("striped", "HOLD_position"), 
+     bootstrap_options = "striped",
+     position = "center",
+     font_size = 9,
+     full_width = FALSE
+   ) %>%
+   footnote(
+     general = "SE Ratio = Clustered SE / Standard SE. Wave 2 p-value changes from 0.009 to 0.056.",
+     footnote_as_chunk = TRUE
+   )
> 
> # For reference, also show the detailed coefficient info for Wave 2
> # Store the detailed output to display it as formatted text
> w2_std <- coef(summary(asp.did.w2))["treat.dum:post.dum",]
> w2_robust <- coeftest(asp.did.w2, vcov = vcovCL(asp.did.w2, cluster = w2_data$Region))["treat.dum:post.dum",]
> 
> # Create a more formatted display of the key coefficient
> coef_detail <- data.frame(
+   "Estimator" = c("Conventional SE", "Cluster-robust SE"),
+   "Estimate" = c(w2_std[1], w2_robust[1]),
+   "Std.Error" = c(w2_std[2], w2_robust[2]),
+   "t-value" = c(w2_std[3], w2_robust[3]),
+   "p-value" = c(w2_std[4], w2_robust[4])
+ )
> 
> # Format the detailed comparison
> coef_detail %>%
+   kbl(
+     caption = "Detailed Comparison for Wave 2 DiD 'Treat x Post' Coefficient",
+     digits = c(0, 3, 3, 2, 3),
+     align = c("l", "r", "r", "r", "r"),
+     booktabs = TRUE
+   ) %>%
+   kable_styling(
+     latex_options = c("striped", "HOLD_position"), 
+     bootstrap_options = "striped",
+     position = "center",
+     font_size = 9,
+     full_width = FALSE
+   )
> # --- End of Robustness Checks Section ---
> message("Robustness check code sections added/corrected.")
Robustness check code sections added/corrected.